{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005d94e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 20:09:15.799430: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-25 20:09:15.802440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-25 20:09:15.802452: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from keras.utils import np_utils,pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bf9576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chicago', 'las-vegas', 'london', 'dubai', 'new-york-city', 'shanghai', 'new-delhi', 'san-francisco', 'montreal', '.DS_Store', 'beijing']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"word2vec_dataset/word2vec_dataset/review_hotels/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0676e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_date_text=[]\n",
    "path=\"word2vec_dataset/word2vec_dataset/review_hotels/data\"\n",
    "for folders in os.listdir(path):\n",
    "    if not folders[0].isalnum():\n",
    "        continue\n",
    "    i=0\n",
    "    for filename in os.listdir(path + '/' + folders):\n",
    "        with open(os.path.join(path + '/' + folders,filename),'r',errors='ignore') as f:\n",
    "            for sentence in f.readlines():\n",
    "                try:\n",
    "                    no_date_text.append(sentence[sentence.index('\\t')+1:])\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15d85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [lemma.lemmatize(word) for word in tokens if word not in stopwords and word.isalpha()]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb08a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255138it [01:40, 2528.18it/s]\n"
     ]
    }
   ],
   "source": [
    "word_lists = []\n",
    "all_words=Counter()\n",
    "maxlen=30\n",
    "\n",
    "for i,text in tqdm(enumerate(no_date_text)):\n",
    "    no_date_text[i] = clean_text(text)[:maxlen+1]\n",
    "    for word in no_date_text[i]:\n",
    "        all_words[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af20a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "uniq_words=[]\n",
    "for w,i in sorted(all_words.items(),key=lambda x:x[1],reverse=True):\n",
    "    if k>9999:\n",
    "        break\n",
    "    uniq_words.append(w)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbb11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 1460361.41it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_word_dict = {}\n",
    "for i, word in tqdm(enumerate(uniq_words)):\n",
    "    unique_word_dict.update({\n",
    "        word: i\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21430e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 255138/255138 [01:12<00:00, 3542.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(len(no_date_text))):\n",
    "    no_date_text[j]=[unique_word_dict[word] for word in no_date_text[j] if word in uniq_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6336e94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[np_utils.to_categorical(2, 10000),np_utils.to_categorical(3, 10000)]\n",
    "np.asarray(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88dd508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, word_lists, unique_word_dict,window_size, batch_size=128,shuffle=False):\n",
    "        'Initialization'\n",
    "        self.word_lists = word_lists\n",
    "        self.batch_size = batch_size\n",
    "        self.unique_word_dict = unique_word_dict\n",
    "        self.window_size=window_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.word_lists) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.word_lists[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.word_lists))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, word_lists_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        n=len(self.unique_word_dict)\n",
    "        x=[]\n",
    "        y=[]\n",
    "        \n",
    "        context_length = self.window_size*2\n",
    "        for words in word_lists_temp:\n",
    "            n1=len(words)\n",
    "            for index, word in enumerate(words):           \n",
    "                start = index - self.window_size\n",
    "                end = index + self.window_size + 1\n",
    "            \n",
    "                context_words=[words[i] for i in range(start, end) if 0 <= i < n1 and i != index]\n",
    "                y.append(np_utils.to_categorical(word, n))\n",
    "\n",
    "                for i in range(context_length-len(context_words)):\n",
    "                    context_words.append(0)\n",
    "                \n",
    "                x.append(context_words)\n",
    "\n",
    "        return np.asarray(x),np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fdd128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45221/611703865.py:16: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1993/1993 [==============================] - 868s 435ms/step - loss: 6.8927\n",
      "Epoch 2/4\n",
      "1993/1993 [==============================] - 877s 439ms/step - loss: 6.2223\n",
      "Epoch 3/4\n",
      "1993/1993 [==============================] - 876s 439ms/step - loss: 5.9522\n",
      "Epoch 4/4\n",
      "1993/1993 [==============================] - 903s 451ms/step - loss: 5.7969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f39ff9570>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "n=len(unique_word_dict)\n",
    "embed_size=100\n",
    "win_size=4\n",
    "data_generator=DataGenerator(no_date_text,unique_word_dict,window_size=win_size,batch_size=128)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n, output_dim=embed_size, input_length=win_size*2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "model.add(Dense(n, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=data_generator,\n",
    "    use_multiprocessing=True,\n",
    "    workers=6,\n",
    "    epochs=4\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55337d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: finalembedd/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('finalembedd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff3bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0]\n",
    "\n",
    "embedding_dict = {}\n",
    "for word in uniq_words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44f5e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol=embedding_dict['polite']\n",
    "dirt=embedding_dict['dirty']\n",
    "fra=embedding_dict['france']\n",
    "sho=embedding_dict['shocked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc17ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def cossim(x,y):\n",
    "    return np.dot(x,y)/norm(x)*norm(y)\n",
    "def top(x):\n",
    "    tops=[]\n",
    "    for w,i in embedding_dict.items():\n",
    "        tops.append([cossim(i,x),w])\n",
    "    tops.sort(reverse=True)\n",
    "    return tops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "212f3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29.635332, 'polite'], [28.015253, 'courteous'], [26.98552, 'cordial'], [26.162766, 'curtious'], [25.99898, 'considerate'], [25.762526, 'freindly'], [25.252836, 'curteous'], [25.22877, 'personable'], [25.153065, 'friendlythe'], [25.011658, 'friendly']]\n"
     ]
    }
   ],
   "source": [
    "print(top(pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "003c76c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40.98794, 'dirty'], [38.460083, 'filthy'], [37.44034, 'smelly'], [36.516823, 'gross'], [36.508984, 'nasty'], [35.86055, 'moldy'], [35.842808, 'unclean'], [35.351227, 'dusty'], [35.21275, 'disgustingly'], [35.139454, 'stinky']]\n"
     ]
    }
   ],
   "source": [
    "print(top(dirt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9736ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.682072, 'france'], [5.254983, 'canada'], [5.065885, 'rajasthan'], [5.0201097, 'spain'], [4.8715787, 'edinburgh'], [4.8692746, 'whirlwind'], [4.760905, 'ca'], [4.7048316, 'agra'], [4.697535, 'singapore'], [4.6870914, 'ireland']]\n"
     ]
    }
   ],
   "source": [
    "print(top(fra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3db9344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.35955, 'shocked'], [13.470577, 'horrified'], [13.263062, 'dismayed'], [12.796984, 'relieved'], [12.638424, 'uncertain'], [12.586542, 'unknown'], [12.508048, 'shock'], [12.492645, 'aback'], [12.316586, 'fear'], [12.043133, 'dissappointed']]\n"
     ]
    }
   ],
   "source": [
    "print(top(sho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613607bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
